"""
AI Insights Module - ìƒì„¸ AI ê¸°ë°˜ ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸
"""
from typing import Dict, Any, List
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import pandas as pd
import numpy as np
from scipy import stats
from . import get_dataframe

router = APIRouter()

class InsightRequest(BaseModel):
    file_id: str
    analysis_type: str = "comprehensive"
    focus_areas: List[str] = []


@router.post("/analysis/ai-insights")
async def generate_ai_insights(request: InsightRequest):
    """ìƒì„¸ AI ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
    try:
        df = get_dataframe(request.file_id)
    except KeyError:
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìƒì„¸ ë¶„ì„ ìˆ˜í–‰
    data_summary = _generate_detailed_summary(df)
    statistical_analysis = _run_statistical_analysis(df)
    quality_report = _assess_data_quality(df)
    insights = _generate_detailed_insights(df, statistical_analysis)
    recommendations = _generate_actionable_recommendations(df, statistical_analysis)
    executive_summary = _generate_executive_summary(df, statistical_analysis, insights)
    
    return {
        "file_id": request.file_id,
        "executive_summary": executive_summary,
        "data_summary": data_summary,
        "quality_report": quality_report,
        "statistical_analysis": statistical_analysis,
        "key_insights": insights,
        "recommendations": recommendations,
        "risk_alerts": _identify_detailed_risks(df, statistical_analysis),
        "opportunities": _identify_opportunities(df, statistical_analysis),
        "next_steps": _suggest_next_steps(df, statistical_analysis)
    }


def _generate_detailed_summary(df: pd.DataFrame) -> Dict:
    """ìƒì„¸ ë°ì´í„° ìš”ì•½"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
    
    # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ í†µê³„
    numeric_stats = []
    for col in numeric_cols[:10]:
        data = df[col].dropna()
        if len(data) > 0:
            numeric_stats.append({
                "column": col,
                "mean": round(float(data.mean()), 2),
                "std": round(float(data.std()), 2),
                "min": round(float(data.min()), 2),
                "max": round(float(data.max()), 2),
                "median": round(float(data.median()), 2),
                "skewness": round(float(data.skew()), 3),
                "range": round(float(data.max() - data.min()), 2)
            })
    
    # ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„í¬
    categorical_stats = []
    for col in categorical_cols[:5]:
        vc = df[col].value_counts()
        categorical_stats.append({
            "column": col,
            "unique_count": int(df[col].nunique()),
            "top_value": str(vc.index[0]) if len(vc) > 0 else None,
            "top_percentage": round(vc.iloc[0] / len(df) * 100, 1) if len(vc) > 0 else 0,
            "distribution": [{"value": str(v), "count": int(c), "pct": round(c/len(df)*100, 1)} for v, c in vc.head(5).items()]
        })
    
    return {
        "total_rows": len(df),
        "total_columns": len(df.columns),
        "numeric_columns": len(numeric_cols),
        "categorical_columns": len(categorical_cols),
        "date_range": None,  # TODO: ë‚ ì§œ ì»¬ëŸ¼ ê°ì§€
        "missing_rate": round(df.isna().mean().mean() * 100, 2),
        "duplicate_rows": int(df.duplicated().sum()),
        "memory_mb": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
        "numeric_stats": numeric_stats,
        "categorical_stats": categorical_stats
    }


def _run_statistical_analysis(df: pd.DataFrame) -> Dict:
    """ê³ ê¸‰ í†µê³„ ë¶„ì„"""
    results = {"correlations": [], "distributions": [], "outliers": [], "trends": []}
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ìƒê´€ê´€ê³„ ë¶„ì„
    if len(numeric_cols) >= 2:
        corr_matrix = df[numeric_cols[:15]].corr()
        for i, c1 in enumerate(numeric_cols[:15]):
            for j, c2 in enumerate(numeric_cols[:15]):
                if i < j:
                    val = corr_matrix.loc[c1, c2]
                    if abs(val) > 0.5:
                        relationship = "ê°•í•œ ì–‘ì˜ ìƒê´€" if val > 0.7 else "ì–‘ì˜ ìƒê´€" if val > 0.5 else "ê°•í•œ ìŒì˜ ìƒê´€" if val < -0.7 else "ìŒì˜ ìƒê´€"
                        results['correlations'].append({
                            'var1': c1, 'var2': c2, 
                            'correlation': round(float(val), 3),
                            'relationship': relationship,
                            'interpretation': f"{c1}ì´(ê°€) ì¦ê°€í•˜ë©´ {c2}ë„ {'ì¦ê°€' if val > 0 else 'ê°ì†Œ'}í•˜ëŠ” ê²½í–¥ (r={val:.3f})"
                        })
    
    # ë¶„í¬ ë¶„ì„
    for col in numeric_cols[:5]:
        data = df[col].dropna()
        if len(data) > 10:
            skew = float(data.skew())
            kurt = float(data.kurtosis())
            dist_type = "ì •ê·œë¶„í¬" if abs(skew) < 0.5 and abs(kurt) < 1 else "ì™¼ìª½ ì¹˜ìš°ì¹¨" if skew < -0.5 else "ì˜¤ë¥¸ìª½ ì¹˜ìš°ì¹¨" if skew > 0.5 else "ë¹„ì •ê·œ"
            results['distributions'].append({
                'column': col,
                'distribution_type': dist_type,
                'skewness': round(skew, 3),
                'kurtosis': round(kurt, 3),
                'interpretation': f"{col}ì€ {dist_type} í˜•íƒœ. {'ë³€í™˜ ê¶Œì¥' if abs(skew) > 1 else 'ë¶„ì„ì— ì í•©'}"
            })
    
    # ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)
    for col in numeric_cols[:5]:
        data = df[col].dropna()
        if len(data) > 10:
            Q1, Q3 = data.quantile(0.25), data.quantile(0.75)
            IQR = Q3 - Q1
            outlier_count = int(((data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)).sum())
            if outlier_count > 0:
                results['outliers'].append({
                    'column': col,
                    'outlier_count': outlier_count,
                    'outlier_percentage': round(outlier_count / len(data) * 100, 2),
                    'lower_bound': round(float(Q1 - 1.5*IQR), 2),
                    'upper_bound': round(float(Q3 + 1.5*IQR), 2),
                    'interpretation': f"{col}ì—ì„œ {outlier_count}ê°œ ì´ìƒì¹˜ ë°œê²¬ ({outlier_count/len(data)*100:.1f}%)"
                })
    
    return results


def _assess_data_quality(df: pd.DataFrame) -> Dict:
    """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
    issues = []
    score = 100
    
    # ê²°ì¸¡ì¹˜ í‰ê°€
    missing_pct = df.isna().mean().mean() * 100
    if missing_pct > 0:
        score -= min(missing_pct * 2, 30)
        severity = "critical" if missing_pct > 20 else "high" if missing_pct > 10 else "medium" if missing_pct > 5 else "low"
        issues.append({
            "type": "missing_data",
            "severity": severity,
            "metric": f"{missing_pct:.1f}%",
            "description": f"ì „ì²´ ë°ì´í„°ì˜ {missing_pct:.1f}%ê°€ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤.",
            "affected_columns": [col for col in df.columns if df[col].isna().sum() > 0][:5],
            "recommendation": "ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•„ìš” (ì‚­ì œ, ëŒ€ì²´, ë³´ê°„ ë“±)"
        })
    
    # ì¤‘ë³µ í‰ê°€
    dup_pct = df.duplicated().sum() / len(df) * 100
    if dup_pct > 0:
        score -= min(dup_pct * 1.5, 20)
        issues.append({
            "type": "duplicates",
            "severity": "high" if dup_pct > 10 else "medium" if dup_pct > 5 else "low",
            "metric": f"{dup_pct:.1f}%",
            "description": f"{int(df.duplicated().sum())}ê°œì˜ ì¤‘ë³µ í–‰ ({dup_pct:.1f}%)",
            "recommendation": "ì¤‘ë³µ ì œê±° ì—¬ë¶€ ê²€í† "
        })
    
    # ë‹¨ì¼ê°’ ì»¬ëŸ¼
    for col in df.columns:
        if df[col].nunique() == 1:
            score -= 5
            issues.append({
                "type": "constant_column",
                "severity": "low",
                "metric": col,
                "description": f"'{col}' ì»¬ëŸ¼ì€ ë‹¨ì¼ ê°’ë§Œ í¬í•¨ (ë¶„ì„ ì˜ë¯¸ ì—†ìŒ)",
                "recommendation": "í•´ë‹¹ ì»¬ëŸ¼ ì œê±° ê³ ë ¤"
            })
    
    return {
        "quality_score": max(0, round(score)),
        "grade": "A" if score >= 90 else "B" if score >= 80 else "C" if score >= 70 else "D" if score >= 60 else "F",
        "issues": issues,
        "total_issues": len(issues)
    }


def _generate_detailed_insights(df: pd.DataFrame, analysis: Dict) -> List[Dict]:
    """ìƒì„¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    insights = []
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ìƒê´€ê´€ê³„ ì¸ì‚¬ì´íŠ¸ (Scatter Plot Data)
    for corr in analysis.get('correlations', [])[:5]:
        if abs(corr['correlation']) > 0.7:
            # Scatter ë°ì´í„° ìƒ˜í”Œë§ (ìµœëŒ€ 50ê°œ)
            try:
                sample_df = df[[corr['var1'], corr['var2']]].dropna().sample(n=min(50, len(df)), random_state=42)
                scatter_data = sample_df.to_dict(orient='records')
            except:
                scatter_data = []

            insights.append({
                "category": "ê´€ê³„ ë°œê²¬",
                "icon": "ğŸ”—",
                "title": f"{corr['var1']}ì™€ {corr['var2']}ì˜ ê°•í•œ ìƒê´€ê´€ê³„",
                "finding": corr['interpretation'],
                "evidence": f"ìƒê´€ê³„ìˆ˜ r={corr['correlation']:.3f}",
                "business_implication": f"{corr['var1']}ì„(ë¥¼) ì¡°ì •í•˜ë©´ {corr['var2']}ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŒ",
                "priority": "high",
                "confidence": round(abs(corr['correlation']) * 100),
                "visualization": {
                    "type": "scatter",
                    "x": corr['var1'],
                    "y": corr['var2'],
                    "data": scatter_data
                }
            })
    
    # ë¶„í¬ ì¸ì‚¬ì´íŠ¸ (Histogram Data)
    for dist in analysis.get('distributions', [])[:3]:
        if dist['distribution_type'] != "ì •ê·œë¶„í¬":
            # íˆìŠ¤í† ê·¸ë¨ ìƒì„± (15 bins)
            try:
                data = df[dist['column']].dropna()
                counts, bin_edges = np.histogram(data, bins=15)
                hist_data = [{"bin": str(round(bin_edges[i], 1)), "count": int(c)} for i, c in enumerate(counts)]
            except:
                hist_data = []

            insights.append({
                "category": "ë¶„í¬ íŠ¹ì„±",
                "icon": "ğŸ“Š",
                "title": f"{dist['column']}ì˜ ë¹„ëŒ€ì¹­ ë¶„í¬",
                "finding": dist['interpretation'],
                "evidence": f"ì™œë„={dist['skewness']:.2f}, ì²¨ë„={dist['kurtosis']:.2f}",
                "business_implication": "í‰ê· ë³´ë‹¤ ì¤‘ì•™ê°’ì´ ë” ëŒ€í‘œì ì¸ ì§€í‘œì¼ ìˆ˜ ìˆìŒ",
                "priority": "medium",
                "confidence": 85,
                "visualization": {
                    "type": "bar",
                    "x": "bin",
                    "y": "count",
                    "label": dist['column'],
                    "data": hist_data
                }
            })
    
    # ì´ìƒì¹˜ ì¸ì‚¬ì´íŠ¸ (Boxplot Summary)
    for outlier in analysis.get('outliers', [])[:3]:
        if outlier['outlier_percentage'] > 1:
            insights.append({
                "category": "ì´ìƒì¹˜ ë°œê²¬",
                "icon": "âš ï¸",
                "title": f"{outlier['column']}ì—ì„œ ì´ìƒì¹˜ ê°ì§€",
                "finding": outlier['interpretation'],
                "evidence": f"ì •ìƒ ë²”ìœ„: {outlier['lower_bound']} ~ {outlier['upper_bound']}",
                "business_implication": "ì´ìƒì¹˜ê°€ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ì¸ì§€ ë°ì´í„° ì˜¤ë¥˜ì¸ì§€ í™•ì¸ í•„ìš”",
                "priority": "high" if outlier['outlier_percentage'] > 5 else "medium",
                "confidence": 90,
                # ì´ìƒì¹˜ëŠ” ë¶„í¬ ì°¨íŠ¸ë¡œë„ ì„¤ëª… ê°€ëŠ¥í•˜ë¯€ë¡œ histogram ì¬ì‚¬ìš©
                "visualization": {
                    "type": "bar",
                    "x": "bin",
                    "y": "count",
                    "label": outlier['column'],
                    "data": [] # ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì•„ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒëµí•˜ê±°ë‚˜ í•„ìš”ì‹œ ì¶”ê°€
                }
            })
    
    # ìƒìœ„ ì»¬ëŸ¼ ì¸ì‚¬ì´íŠ¸ (Histogram)
    if numeric_cols:
        for col in numeric_cols[:2]:
            data = df[col].dropna()
            if len(data) > 0:
                try:
                    counts, bin_edges = np.histogram(data, bins=15)
                    hist_data = [{"bin": str(round(bin_edges[i], 1)), "count": int(c)} for i, c in enumerate(counts)]
                except:
                    hist_data = []

                insights.append({
                    "category": "í•µì‹¬ ì§€í‘œ",
                    "icon": "ğŸ“ˆ",
                    "title": f"{col} ë¶„ì„ ê²°ê³¼",
                    "finding": f"í‰ê·  {data.mean():,.2f}, ì¤‘ì•™ê°’ {data.median():,.2f}, í‘œì¤€í¸ì°¨ {data.std():,.2f}",
                    "evidence": f"ë°ì´í„° ë²”ìœ„: {data.min():,.2f} ~ {data.max():,.2f}",
                    "business_implication": f"{'í¸ì°¨ê°€ í¼ - ì„¸ë¶„í™” ë¶„ì„ ê¶Œì¥' if data.std() > data.mean() * 0.5 else 'ì•ˆì •ì ì¸ ë¶„í¬'}",
                    "priority": "medium",
                    "confidence": 95,
                    "visualization": {
                        "type": "bar",
                        "x": "bin",
                        "y": "count",
                        "label": col,
                        "data": hist_data
                    }
                })
    
    return insights


def _generate_actionable_recommendations(df: pd.DataFrame, analysis: Dict) -> List[Dict]:
    """ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­"""
    recommendations = []
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
    
    # ìƒê´€ê´€ê³„ ê¸°ë°˜ ì¶”ì²œ
    if analysis.get('correlations'):
        top_corr = analysis['correlations'][0]
        recommendations.append({
            "category": "ì˜ˆì¸¡ ëª¨ë¸",
            "icon": "ğŸ¤–",
            "title": f"{top_corr['var2']} ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•",
            "description": f"{top_corr['var1']}ì„(ë¥¼) í™œìš©í•˜ì—¬ {top_corr['var2']}ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "rationale": f"ë‘ ë³€ìˆ˜ ê°„ ìƒê´€ê³„ìˆ˜ {top_corr['correlation']:.3f}ë¡œ ì˜ˆì¸¡ë ¥ í™•ë³´ ê°€ëŠ¥",
            "action_items": [
                f"1. {top_corr['var1']} ë°ì´í„° ìˆ˜ì§‘ ê°•í™”",
                f"2. ì„ í˜• íšŒê·€ ëª¨ë¸ë¡œ {top_corr['var2']} ì˜ˆì¸¡",
                "3. ì¶”ê°€ ë³€ìˆ˜ í¬í•¨í•˜ì—¬ ë‹¤ì¤‘ íšŒê·€ ë¶„ì„"
            ],
            "expected_impact": "ì˜ˆì¸¡ ì •í™•ë„ 70% ì´ìƒ ê¸°ëŒ€",
            "effort": "ì¤‘ê°„",
            "priority": "high"
        })
    
    # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ì¶”ì²œ
    if cat_cols:
        recommendations.append({
            "category": "ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„",
            "icon": "ğŸ¯",
            "title": f"{cat_cols[0]} ê¸°ì¤€ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„",
            "description": f"'{cat_cols[0]}' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ë³„ íŠ¹ì„±ì„ ë¹„êµí•˜ì„¸ìš”.",
            "rationale": f"'{cat_cols[0]}'ëŠ” {df[cat_cols[0]].nunique()}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜ ê°€ëŠ¥",
            "action_items": [
                f"1. '{cat_cols[0]}' ì„¸ê·¸ë¨¼íŠ¸ íƒ­ì—ì„œ ë¶„ì„",
                "2. ê·¸ë£¹ë³„ ìˆ˜ì¹˜ ì§€í‘œ í‰ê·  ë¹„êµ",
                "3. ìœ ì˜í•œ ì°¨ì´ê°€ ìˆëŠ” ê·¸ë£¹ ì‹ë³„"
            ],
            "expected_impact": "ê·¸ë£¹ë³„ ë§ì¶¤ ì „ëµ ìˆ˜ë¦½",
            "effort": "ë‚®ìŒ",
            "priority": "high"
        })
    
    # A/B í…ŒìŠ¤íŠ¸ ì¶”ì²œ
    binary_cols = [c for c in cat_cols if df[c].nunique() == 2]
    if binary_cols and numeric_cols:
        recommendations.append({
            "category": "A/B í…ŒìŠ¤íŠ¸",
            "icon": "ğŸ§ª",
            "title": f"{binary_cols[0]} ê¸°ì¤€ A/B í…ŒìŠ¤íŠ¸",
            "description": f"'{binary_cols[0]}' ë‘ ê·¸ë£¹ ê°„ ì§€í‘œ ì°¨ì´ë¥¼ ê²€ì •í•˜ì„¸ìš”.",
            "rationale": f"2ê°œ ê·¸ë£¹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ë‚˜ë‰˜ì–´ A/B í…ŒìŠ¤íŠ¸ì— ì í•©",
            "action_items": [
                f"1. A/B í…ŒìŠ¤íŠ¸ íƒ­ì—ì„œ '{binary_cols[0]}' ì„ íƒ",
                f"2. '{numeric_cols[0]}' ë“± ìˆ˜ì¹˜ ì§€í‘œ ë¹„êµ",
                "3. í†µê³„ì  ìœ ì˜ì„± í™•ì¸ (p-value < 0.05)"
            ],
            "expected_impact": "ì˜ì‚¬ê²°ì • ê·¼ê±° í™•ë³´",
            "effort": "ë‚®ìŒ",
            "priority": "high"
        })
    
    # ì‹œê³„ì—´ ë¶„ì„ ì¶”ì²œ
    date_cols = [c for c in df.columns if 'date' in c.lower() or 'ë‚ ì§œ' in c or 'ì¼ì' in c]
    if date_cols and numeric_cols:
        recommendations.append({
            "category": "ì‹œê³„ì—´ ë¶„ì„",
            "icon": "ğŸ“ˆ",
            "title": "ì‹œê°„ì— ë”°ë¥¸ ì¶”ì„¸ ë¶„ì„",
            "description": f"'{date_cols[0]}'ì„(ë¥¼) ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ëŒ€ë³„ ë³€í™”ë¥¼ ë¶„ì„í•˜ì„¸ìš”.",
            "rationale": "ì‹œê³„ì—´ ë°ì´í„°ë¡œ ì¶”ì„¸ì™€ ê³„ì ˆì„± íŒŒì•… ê°€ëŠ¥",
            "action_items": [
                f"1. ì‹œê³„ì—´ íƒ­ì—ì„œ '{date_cols[0]}' ì„ íƒ",
                f"2. '{numeric_cols[0]}' ê°’ ì»¬ëŸ¼ ì„ íƒ",
                "3. ì›”ë³„/ì£¼ë³„ íŒ¨í„´ ë° ì˜ˆì¸¡ í™•ì¸"
            ],
            "expected_impact": "ë¯¸ë˜ ì¶”ì„¸ ì˜ˆì¸¡",
            "effort": "ë‚®ìŒ",
            "priority": "medium"
        })
    
    return recommendations


def _identify_detailed_risks(df: pd.DataFrame, analysis: Dict) -> List[Dict]:
    """ìƒì„¸ ë¦¬ìŠ¤í¬ ì‹ë³„"""
    risks = []
    
    missing_pct = df.isna().mean().mean() * 100
    if missing_pct > 10:
        risks.append({
            "type": "data_quality",
            "severity": "high",
            "icon": "ğŸš¨",
            "title": "ë†’ì€ ê²°ì¸¡ë¥ ",
            "description": f"ì „ì²´ ë°ì´í„°ì˜ {missing_pct:.1f}%ê°€ ê²°ì¸¡ì¹˜ë¡œ, ë¶„ì„ ì‹ ë¢°ë„ê°€ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "impact": "ë¶„ì„ ê²°ê³¼ ì™œê³¡, ëª¨ë¸ ì„±ëŠ¥ ì €í•˜",
            "mitigation": "ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì‚­ì œ, í‰ê· /ì¤‘ì•™ê°’ ëŒ€ì²´, ë‹¤ì¤‘ëŒ€ì…ë²•)"
        })
    
    if len(df) < 100:
        risks.append({
            "type": "statistical",
            "severity": "medium",
            "icon": "âš ï¸",
            "title": "ì‘ì€ í‘œë³¸ í¬ê¸°",
            "description": f"ì´ {len(df)}ê±´ì˜ ë°ì´í„°ë¡œ í†µê³„ì  ìœ ì˜ì„± í™•ë³´ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "impact": "ê²€ì •ë ¥ ë¶€ì¡±, ì‹ ë¢°êµ¬ê°„ í™•ëŒ€",
            "mitigation": "ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ ê¶Œì¥ (ìµœì†Œ 100~500ê±´)"
        })
    
    if analysis.get('outliers') and sum(o['outlier_count'] for o in analysis['outliers']) > len(df) * 0.1:
        risks.append({
            "type": "outliers",
            "severity": "medium",
            "icon": "ğŸ“",
            "title": "ë‹¤ìˆ˜ì˜ ì´ìƒì¹˜",
            "description": "10% ì´ìƒì˜ ë°ì´í„°ê°€ ì´ìƒì¹˜ë¡œ ë¶„ë¥˜ë¨",
            "impact": "í‰ê· /í‘œì¤€í¸ì°¨ ì™œê³¡",
            "mitigation": "ì´ìƒì¹˜ ì›ì¸ íŒŒì•… í›„ ì²˜ë¦¬ ì—¬ë¶€ ê²°ì •"
        })
    
    return risks


def _identify_opportunities(df: pd.DataFrame, analysis: Dict) -> List[Dict]:
    """ê¸°íšŒ ìš”ì†Œ ì‹ë³„"""
    opportunities = []
    
    if analysis.get('correlations'):
        strong_corrs = [c for c in analysis['correlations'] if abs(c['correlation']) > 0.7]
        if strong_corrs:
            opportunities.append({
                "type": "prediction",
                "icon": "ğŸ’¡",
                "title": "ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ê¸°íšŒ",
                "description": f"{len(strong_corrs)}ê°œì˜ ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬ - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— í™œìš© ê°€ëŠ¥",
                "potential_impact": "ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„ ë‹¬ì„± ê°€ëŠ¥"
            })
    
    cat_cols = df.select_dtypes(exclude=[np.number]).columns
    for col in cat_cols:
        if 2 <= df[col].nunique() <= 10:
            opportunities.append({
                "type": "segmentation",
                "icon": "ğŸ¯",
                "title": f"'{col}' ê¸°ë°˜ ì„¸ë¶„í™”",
                "description": f"{df[col].nunique()}ê°œ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ëª…í™•í•œ ê·¸ë£¹ ë¶„ì„ ê°€ëŠ¥",
                "potential_impact": "íƒ€ê²Ÿ ê·¸ë£¹ë³„ ë§ì¶¤ ì „ëµ"
            })
            break
    
    return opportunities


def _suggest_next_steps(df: pd.DataFrame, analysis: Dict) -> List[Dict]:
    """ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"""
    steps = []
    cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if cat_cols and num_cols:
        binary = [c for c in cat_cols if df[c].nunique() == 2]
        if binary:
            steps.append({
                "step": 1,
                "action": "A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰",
                "detail": f"'{binary[0]}' ê·¸ë£¹ë³„ '{num_cols[0]}' ë¹„êµ",
                "icon": "ğŸ§ª"
            })
    
    steps.append({"step": 2, "action": "ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„", "detail": "ê·¸ë£¹ë³„ íŠ¹ì„± ë¹„êµ", "icon": "ğŸ¯"})
    steps.append({"step": 3, "action": "ì‹œê³„ì—´ ë¶„ì„", "detail": "ì¶”ì„¸ ë° ì˜ˆì¸¡", "icon": "ğŸ“ˆ"})
    steps.append({"step": 4, "action": "ìƒê´€ê´€ê³„ ì‹¬ì¸µ ë¶„ì„", "detail": "ì¸ê³¼ê´€ê³„ ê²€ì¦", "icon": "ğŸ”—"})
    
    return steps


def _generate_executive_summary(df: pd.DataFrame, analysis: Dict, insights: List[Dict]) -> str:
    """ê²½ì˜ì§„ìš© ìš”ì•½ ë¦¬í¬íŠ¸"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
    
    summary_parts = []
    
    # ë°ì´í„° ê·œëª¨
    summary_parts.append(f"ğŸ“Š **ë°ì´í„° ê°œìš”**: ì´ {len(df):,}ê±´, {len(df.columns)}ê°œ ì»¬ëŸ¼ (ìˆ˜ì¹˜í˜• {len(numeric_cols)}ê°œ, ë²”ì£¼í˜• {len(cat_cols)}ê°œ)")
    
    # í’ˆì§ˆ í˜„í™©
    missing_pct = df.isna().mean().mean() * 100
    dup_cnt = df.duplicated().sum()
    quality = "ì–‘í˜¸" if missing_pct < 5 and dup_cnt < len(df)*0.01 else "ê°œì„  í•„ìš”"
    summary_parts.append(f"âœ… **ë°ì´í„° í’ˆì§ˆ**: {quality} (ê²°ì¸¡ {missing_pct:.1f}%, ì¤‘ë³µ {dup_cnt}ê±´)")
    
    # í•µì‹¬ ë°œê²¬
    if analysis.get('correlations'):
        top = analysis['correlations'][0]
        summary_parts.append(f"ğŸ”— **í•µì‹¬ ë°œê²¬**: {top['var1']}ì™€ {top['var2']} ê°„ {top['relationship']} (r={top['correlation']:.2f})")
    
    # ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìˆ˜
    high_priority = len([i for i in insights if i.get('priority') == 'high'])
    summary_parts.append(f"ğŸ’¡ **ì¸ì‚¬ì´íŠ¸**: ì´ {len(insights)}ê°œ ë°œê²¬ (ì¤‘ìš” {high_priority}ê°œ)")
    
    # ì¶”ì²œ ì•¡ì…˜
    if cat_cols:
        summary_parts.append(f"ğŸ¯ **ì¶”ì²œ ì•¡ì…˜**: '{cat_cols[0]}' ê¸°ì¤€ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ìœ¼ë¡œ ê·¸ë£¹ë³„ íŠ¹ì„± íŒŒì•…")
    
    return "\n\n".join(summary_parts)
