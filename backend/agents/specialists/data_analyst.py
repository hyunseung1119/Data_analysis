"""
DataAnalystAgent - ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸

CSV ë°ì´í„° ìë™ ë¶„ì„, ìƒê´€ê´€ê³„, A/B í…ŒìŠ¤íŠ¸ ë“±ì„ ë‹´ë‹¹.
"""
import io
import json
from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np
from scipy import stats

from ..base import BaseAgent, AgentInput, AgentOutput, Visualization
from ..registry import AgentRegistry


@AgentRegistry.register("data_analyst", config={
    "description": "CSV ë°ì´í„° ìë™ ë¶„ì„ ì „ë¬¸ê°€",
    "temperature": 0.2,
})
class DataAnalystAgent(BaseAgent):
    """
    ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸
    
    Capabilities:
    - CSV ìë™ í”„ë¡œíŒŒì¼ë§
    - ìƒê´€ê´€ê³„ ë¶„ì„
    - A/B í…ŒìŠ¤íŠ¸
    - ê¸°ìˆ  í†µê³„
    - ì¸ì‚¬ì´íŠ¸ ìƒì„±
    """
    
    def __init__(
        self, 
        name: str = "data_analyst",
        description: str = "CSV ë°ì´í„° ìë™ ë¶„ì„ ì „ë¬¸ê°€",
        llm_service = None,
        config: Dict[str, Any] = None
    ):
        super().__init__(name, description, llm_service, config)
        self._current_df: Optional[pd.DataFrame] = None
    
    def get_system_prompt(self) -> str:
        return """ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
CSV ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.

ê·œì¹™:
- ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ ì‹ë³„
- ì£¼ìš” íŒ¨í„´ê³¼ ìƒê´€ê´€ê³„ ë°œê²¬
- ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ
- ì‹œê°í™” ê¶Œì¥ì‚¬í•­ ì œì‹œ

ì¶œë ¥ í˜•ì‹:
## ğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼

### ë°ì´í„° ê°œìš”
| í•­ëª© | ê°’ |
|------|-----|
| í–‰ ìˆ˜ | ... |
| ì—´ ìˆ˜ | ... |
| ê²°ì¸¡ì¹˜ | ... |

### ì£¼ìš” ë°œê²¬ì‚¬í•­
1. [ë°œê²¬ì‚¬í•­ 1]
2. [ë°œê²¬ì‚¬í•­ 2]

### ìƒê´€ê´€ê³„ ë¶„ì„
[ê°•í•œ ìƒê´€ê´€ê³„ ì„¤ëª…]

### ì¸ì‚¬ì´íŠ¸ ë° ê¶Œì¥ì‚¬í•­
[ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ]
"""
    
    async def execute(self, input: AgentInput) -> AgentOutput:
        """ë°ì´í„° ë¶„ì„ ì‹¤í–‰"""
        visualizations = []
        
        # CSV ë°ì´í„°ê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸
        csv_data = input.context.get("csv_data")
        file_path = input.context.get("file_path")
        
        if csv_data or file_path:
            # CSV ë¡œë“œ ë° ë¶„ì„
            try:
                if csv_data:
                    self._current_df = pd.read_csv(io.StringIO(csv_data))
                elif file_path:
                    self._current_df = pd.read_csv(file_path)
                
                # ê¸°ë³¸ í”„ë¡œíŒŒì¼ë§
                profile = self._profile_data(self._current_df)
                
                # ìƒê´€ê´€ê³„ ë¶„ì„
                correlation = self._analyze_correlation(self._current_df)
                
                # ì‹œê°í™” ë°ì´í„° ìƒì„±
                if correlation:
                    visualizations.append(Visualization(
                        type="heatmap",
                        title="ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ",
                        data=correlation["matrix_data"],
                        insight=correlation["insight"]
                    ))
                
                # ë¶„í¬ ì‹œê°í™”
                dist_viz = self._create_distribution_viz(self._current_df)
                if dist_viz:
                    visualizations.append(dist_viz)
                
                # LLMìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
                if self.llm:
                    insight_prompt = f"""ë‹¤ìŒ ë°ì´í„° í”„ë¡œíŒŒì¼ì„ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{json.dumps(profile, indent=2, ensure_ascii=False)}

ìƒê´€ê´€ê³„ ë¶„ì„:
{json.dumps(correlation, indent=2, ensure_ascii=False) if correlation else 'ìˆ˜ì¹˜í˜• ë°ì´í„° ì—†ìŒ'}

ì‚¬ìš©ì ì§ˆë¬¸: {input.query}
"""
                    result = await self.llm.chat(
                        system_prompt=self.get_system_prompt(),
                        user_message=insight_prompt
                    )
                else:
                    result = self._generate_basic_report(profile, correlation)
                
                return AgentOutput(
                    agent_name=self.name,
                    result=result,
                    confidence=0.85,
                    reasoning="CSV ë°ì´í„° ìë™ ë¶„ì„ ì™„ë£Œ",
                    metadata={
                        "rows": len(self._current_df),
                        "columns": len(self._current_df.columns),
                        "profile": profile
                    },
                    visualizations=visualizations
                )
                
            except Exception as e:
                return AgentOutput(
                    agent_name=self.name,
                    result=f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                    confidence=0.0,
                    reasoning=str(e)
                )
        
        # CSV ë°ì´í„° ì—†ì´ ì¼ë°˜ ë¶„ì„ ì§ˆë¬¸
        if self.llm:
            result = await self.llm.chat(
                system_prompt=self.get_system_prompt(),
                user_message=f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë°ì´í„° ë¶„ì„ ê´€ì ì—ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:\n\n{input.query}"
            )
            return AgentOutput(
                agent_name=self.name,
                result=result,
                confidence=0.75,
                reasoning="ì¼ë°˜ ë°ì´í„° ë¶„ì„ ì¡°ì–¸ ì œê³µ"
            )
        
        return AgentOutput(
            agent_name=self.name,
            result="CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë°ì´í„° ë¶„ì„ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.",
            confidence=0.5,
            reasoning="ë°ì´í„° ì—†ìŒ"
        )
    
    def _profile_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ê¸°ë³¸ ë°ì´í„° í”„ë¡œíŒŒì¼ë§"""
        profile = {
            "shape": {"rows": len(df), "columns": len(df.columns)},
            "columns": [],
            "missing_values": {},
            "data_quality": {}
        }
        
        for col in df.columns:
            col_info = {
                "name": col,
                "dtype": str(df[col].dtype),
                "missing": int(df[col].isna().sum()),
                "missing_pct": round(df[col].isna().mean() * 100, 2),
                "unique": int(df[col].nunique())
            }
            
            if pd.api.types.is_numeric_dtype(df[col]):
                col_info.update({
                    "mean": round(df[col].mean(), 2) if not df[col].isna().all() else None,
                    "std": round(df[col].std(), 2) if not df[col].isna().all() else None,
                    "min": round(df[col].min(), 2) if not df[col].isna().all() else None,
                    "max": round(df[col].max(), 2) if not df[col].isna().all() else None,
                })
            
            profile["columns"].append(col_info)
            
            if col_info["missing"] > 0:
                profile["missing_values"][col] = col_info["missing_pct"]
        
        # ë°ì´í„° í’ˆì§ˆ ê²½ê³ 
        warnings = []
        if df.duplicated().sum() > 0:
            warnings.append(f"ì¤‘ë³µ í–‰ {df.duplicated().sum()}ê°œ ë°œê²¬")
        
        high_missing = [c for c in profile["columns"] if c["missing_pct"] > 20]
        if high_missing:
            warnings.append(f"ê²°ì¸¡ì¹˜ 20% ì´ˆê³¼ ì»¬ëŸ¼: {[c['name'] for c in high_missing]}")
        
        profile["data_quality"]["warnings"] = warnings
        
        return profile
    
    def _analyze_correlation(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(numeric_cols) < 2:
            return None
        
        corr_matrix = df[numeric_cols].corr()
        
        # ê°•í•œ ìƒê´€ê´€ê³„ ì¶”ì¶œ
        strong_correlations = []
        for i, col1 in enumerate(numeric_cols):
            for j, col2 in enumerate(numeric_cols):
                if i < j:
                    corr_val = corr_matrix.loc[col1, col2]
                    if abs(corr_val) > 0.7:
                        strong_correlations.append({
                            "pair": [col1, col2],
                            "correlation": round(corr_val, 3),
                            "strength": "ê°•í•œ ì–‘ì˜ ìƒê´€" if corr_val > 0 else "ê°•í•œ ìŒì˜ ìƒê´€"
                        })
        
        # íˆíŠ¸ë§µìš© ë°ì´í„° ë³€í™˜
        matrix_data = []
        for col in numeric_cols:
            row_data = {"id": col}
            for col2 in numeric_cols:
                row_data[col2] = round(corr_matrix.loc[col, col2], 2)
            matrix_data.append(row_data)
        
        insight = ""
        if strong_correlations:
            pairs = [f"{c['pair'][0]} â†” {c['pair'][1]} (r={c['correlation']})" for c in strong_correlations[:3]]
            insight = f"ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬: {', '.join(pairs)}"
        
        return {
            "matrix_data": matrix_data,
            "strong_correlations": strong_correlations,
            "insight": insight
        }
    
    def _create_distribution_viz(self, df: pd.DataFrame) -> Optional[Visualization]:
        """ë¶„í¬ ì‹œê°í™” ë°ì´í„° ìƒì„±"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_cols:
            return None
        
        # ì²« ë²ˆì§¸ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ë¶„í¬
        col = numeric_cols[0]
        data = df[col].dropna()
        
        # íˆìŠ¤í† ê·¸ë¨ ë°ì´í„°
        hist, bin_edges = np.histogram(data, bins=10)
        hist_data = [
            {"bin": f"{bin_edges[i]:.1f}-{bin_edges[i+1]:.1f}", "count": int(hist[i])}
            for i in range(len(hist))
        ]
        
        return Visualization(
            type="bar",
            title=f"{col} ë¶„í¬",
            data=hist_data,
            insight=f"í‰ê· : {data.mean():.2f}, í‘œì¤€í¸ì°¨: {data.std():.2f}"
        )
    
    def _generate_basic_report(self, profile: Dict, correlation: Optional[Dict]) -> str:
        """ê¸°ë³¸ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = f"""## ğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼

### ë°ì´í„° ê°œìš”
| í•­ëª© | ê°’ |
|:-----|:---|
| í–‰ ìˆ˜ | {profile['shape']['rows']:,} |
| ì—´ ìˆ˜ | {profile['shape']['columns']} |
| ê²°ì¸¡ì¹˜ ìˆëŠ” ì»¬ëŸ¼ | {len(profile['missing_values'])}ê°œ |

### ì»¬ëŸ¼ ì •ë³´
| ì»¬ëŸ¼ëª… | íƒ€ì… | ê²°ì¸¡ì¹˜(%) | ê³ ìœ ê°’ |
|:------|:-----|--------:|------:|
"""
        for col in profile['columns'][:10]:
            report += f"| {col['name']} | {col['dtype']} | {col['missing_pct']}% | {col['unique']} |\n"
        
        if profile['data_quality']['warnings']:
            report += "\n### âš ï¸ ë°ì´í„° í’ˆì§ˆ ê²½ê³ \n"
            for w in profile['data_quality']['warnings']:
                report += f"- {w}\n"
        
        if correlation and correlation['strong_correlations']:
            report += "\n### ğŸ“ˆ ìƒê´€ê´€ê³„ ë¶„ì„\n"
            for c in correlation['strong_correlations'][:5]:
                emoji = "â¬†ï¸" if c['correlation'] > 0 else "â¬‡ï¸"
                report += f"- {c['pair'][0]} â†” {c['pair'][1]}: r={c['correlation']} {emoji}\n"
        
        return report
    
    async def perform_ab_test(
        self, 
        df: pd.DataFrame,
        group_col: str,
        metric_col: str,
        alpha: float = 0.05
    ) -> Dict[str, Any]:
        """A/B í…ŒìŠ¤íŠ¸ ìˆ˜í–‰"""
        groups = df[group_col].unique()
        
        if len(groups) != 2:
            return {"error": "A/B í…ŒìŠ¤íŠ¸ì—ëŠ” ì •í™•íˆ 2ê°œ ê·¸ë£¹ì´ í•„ìš”í•©ë‹ˆë‹¤."}
        
        group_a = df[df[group_col] == groups[0]][metric_col].dropna()
        group_b = df[df[group_col] == groups[1]][metric_col].dropna()
        
        # t-test
        t_stat, p_value = stats.ttest_ind(group_a, group_b)
        
        # íš¨ê³¼ í¬ê¸° (Cohen's d)
        pooled_std = np.sqrt((group_a.std()**2 + group_b.std()**2) / 2)
        effect_size = (group_b.mean() - group_a.mean()) / pooled_std if pooled_std > 0 else 0
        
        is_significant = p_value < alpha
        
        return {
            "group_a": {"name": str(groups[0]), "mean": group_a.mean(), "n": len(group_a)},
            "group_b": {"name": str(groups[1]), "mean": group_b.mean(), "n": len(group_b)},
            "t_statistic": t_stat,
            "p_value": p_value,
            "effect_size": effect_size,
            "is_significant": is_significant,
            "conclusion": f"í†µê³„ì ìœ¼ë¡œ {'ìœ ì˜í•¨ âœ…' if is_significant else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ âš ï¸'} (p={p_value:.4f})"
        }
